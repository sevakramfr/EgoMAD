# Egocentric Video Domain Adaptation

## Introduction
This repository includes the PyTorch implementation of the Multi-dataset Deep Dictionary Network (MD<sup>2</sup>Net) framework for Unsupervised Domain Adaptation (UDA) in fine-grained egocentric action recognition. The framework is built on multiple datasets. We test the framework using publicly available UDA benchmarks like EPIC-8 [1], ADL-7, and GTEA_KITCHEN-6 [2], along with proposed MSUDA benchmarks.



## Datasets
#### EPIC-8
Download RGB frames from participants P01, P08 and P22 of the EPIC-KITCHENS-55 dataset, using official download script [Epic-kitchens](https://github.com/epic-kitchens/epic-kitchens-download-scripts)

#### ADL-7 and GTEA_KITCHEN-6
Download RGB frames for ADL-7 and GTEA_KITCHEN-6 by following the instructions in [EgoAction](https://github.com/XianyuanLiu/EgoAction).

## Benchmark
For proposed MSUDA benchmarks refer [Annotation](https://github.com/sevakramfr/MDDNet/tree/main/annotation).

## Code
The code and model will be available soon...

## References
1. J. Munro and D. Damen, “Multi-modal domain adaptation for finegrained action recognition,” in Proc. IEEE Int. Conf. Comput. Vis. Work., pp. 3723–3726, Oct. 2019.
2. X. Liu, S. Zhou, T. Lei, P. Jiang, Z. Chen, and H. Lu, “First-person video domain adaptation with multi-scene cross-site datasets and attentionbased methods,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 12, pp. 7774–7788, Dec. 2023.
