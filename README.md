# Egocentric Video Domain Adaptation

## Introduction
This repository contains the PyTorch implementation of the MDDNet framework for UDA approach in fine-grained action recognition based on multiple datasets. The framework is experimented on publicly available UDA (EPIC-8 [1], ADL-7, and GTEA_KITCHEN-6[2]), and proposed MSUDA benchmarks.

## Datasets
### EPIC-8
Download RGB frames from participants P01, P08 and P22 of the EPIC-KITCHENS-55 dataset, using official download script [Here](https://github.com/epic-kitchens/epic-kitchens-download-scripts)

### ADL-7 and GTEA_KITCHEN-6
Follow the instructions in [EgoAction](https://github.com/XianyuanLiu/EgoAction).

## Code
The code and model will be available soon...

## References
1. J. Munro and D. Damen, “Multi-modal domain adaptation for finegrained action recognition,” in Proc. IEEE Int. Conf. Comput. Vis. Work., Oct. 2019, pp. 3723–3726.
2. X. Liu, S. Zhou, T. Lei, P. Jiang, Z. Chen, and H. Lu, “First-person video domain adaptation with multi-scene cross-site datasets and attentionbased methods,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 12, pp. 7774–7788, Dec. 2023.
