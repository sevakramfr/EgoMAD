We present two distinct sets of Multi-Source Unsupervised Domain Adaptation (MSUDA) benchmarks for fine-grained egocentric action recognition. 

The benchmarks are useful for cross-domain and cross-dataset evaluation across a wide range of kitchen and daily living environments.

The dataset contains clips selected from EPIC-Kitchens, ADL, GTEA and Kitchen datasets. 


## References
[1]. F. de la Torre, J. K. Hodgins, J. Montano, and S. Valcarcel, “Detailed human data acquisition of kitchen activities: the CMU-multimodal activity database (CMU-MMAC),” in Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI) Workshop, 2009.

[2]. A. Fathi, X. Ren, and J. M. Rehg, “Learning to recognize objects in egocentric activities,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3281–3288, Jun. 2011

[3]. H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in first-person camera views,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2847–2854, Jun. 2012.

[4]. J. Munro and D. Damen, “Multi-modal domain adaptation for finegrained action recognition,” in Proc. IEEE Int. Conf. Comput. Vis. Work., pp. 3723–3726, Oct. 2019.

[5]. D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “The epic-kitchens dataset: Collection, challenges and baselines,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 11, pp. 4125–4141, Nov. 2021.

[6]. X. Liu, S. Zhou, T. Lei, P. Jiang, Z. Chen, and H. Lu, “First-person video domain adaptation with multi-scene cross-site datasets and attentionbased methods,” IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 12, pp. 7774–7788, Dec. 2023.
