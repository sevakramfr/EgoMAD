# Introduction
We have compiled the existing UDA benchmarks EPIC-8 [3], ADL-7 and GTEA_KITCHEN-6 [1] for ready reference to the researchers. We would like to thank the original authors for their wonderful contributions. Additionally, we present two distinct sets of Multi-Source Unsupervised Domain Adaptation (MSUDA) benchmarks for fine-grained egocentric action recognition for future research in this area. The details of the benchmark is available at [Benchmark](https://github.com/sevakramfr/MDDNet/tree/main/annotation/MSUDA).

# References
1. X. Liu, S. Zhou, T. Lei, P. Jiang, Z. Chen, and H. Lu, “First-person video domain adaptation with multi-scene cross-site datasets and attentionbased methods,” IEEE Trans. Circuits Syst. 
     Video Technol., vol. 33, no. 12, pp. 7774–7788, Dec. 2023.

2. D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “The epic-kitchens dataset: Collection, challenges 
     and baselines,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 11, pp. 4125–4141, Nov. 2021.

3. J. Munro and D. Damen, “Multi-modal domain adaptation for finegrained action recognition,” in Proc. IEEE Int. Conf. Comput. Vis. Work., pp. 3723–3726, Oct. 2019.

4. H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in first-person camera views,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2847–2854, Jun. 2012.

5. A. Fathi, X. Ren, and J. M. Rehg, “Learning to recognize objects in egocentric activities,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3281–3288, Jun. 2011

6. F. de la Torre, J. K. Hodgins, J. Montano, and S. Valcarcel, “Detailed human data acquisition of kitchen activities: the CMU-multimodal activity database (CMU-MMAC),” in Proceedings of 
     the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI) Workshop, 2009.
